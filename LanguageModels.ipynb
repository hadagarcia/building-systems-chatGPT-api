{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh2pozYiSHZGTVaY8KdJrX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadagarcia/building-systems-chatGPT-api/blob/main/LanguageModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uiqk9LSdWZJ"
      },
      "outputs": [],
      "source": [
        "# Setup:\n",
        "!pip install openai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv"
      ],
      "metadata": {
        "id": "zscirz9Idwdf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find a better way to store the key and retrieve it.\n",
        "load_dotenv('/content/sample_data/mis_llaves.env')\n",
        "openai.api_key = os.getenv('OPEN_API_KEY')"
      ],
      "metadata": {
        "id": "L004JAgxd1iK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "9NmIMeryeQ3b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are two types of Large Language Models (LLMs)**\n",
        "\n",
        "\n",
        "*   **Base LLM**: Predicts text word, based on text training data.\n",
        "*   **Instruction Tuned LLM**: Tries to follow instructions.\n",
        "\n"
      ],
      "metadata": {
        "id": "bYH_p7o_e6of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pormpt the model and get a completion"
      ],
      "metadata": {
        "id": "KXE1aeHteZ85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"What is the capital of Germany?\")"
      ],
      "metadata": {
        "id": "YuaYyQJEedEz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abW9dOCsekOG",
        "outputId": "9601dea4-74d3-45e9-ae26-b651431d0267"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Germany is Berlin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From a Base LLM to an instruction tuned LLM:**\n",
        "\n",
        "1.   Train a Base LLM on a lot of data.\n",
        "2.   Further train the model:\n",
        "\n",
        "*   Fine-tune on examples of where the output follows an input instruction.\n",
        "*   Obtain human-ratings of the quality of different LLM outputs, based on criterias like: is it helpful, honest and harmless?\n",
        "*   Tune LLM to increase probability that it generates the more highly rated outputs, using RLHF: Reinforcement Learning from Human Feedback.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xvLhnVJ8f3J2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokens**\n",
        "\n",
        "LLMs don't actually predict the next word, but the next tokens.\n",
        "\n",
        "Different Models have different limits on the number of tokens in the input \"context\" + output completion. GPT-3.5-Turbo has ~4000 tokens.\n",
        "\n",
        "Here are a couple of examples, one where we're asking a task to be executed based on a word and another one using a token approach..."
      ],
      "metadata": {
        "id": "RHHqjQdThVxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"Take the letters in lollipop \\\n",
        "and reverse them\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MM-3pSXhIG3",
        "outputId": "8d752d22-61c9-4b4b-b890-3d4583d7e052"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ppilolol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"\"\"Take the letters in \\\n",
        "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaU1AUqHiN0L",
        "outputId": "3a6329f2-d77f-427d-eb1b-2053780baa75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-o-p-i-l-l-o-l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper function (chat format)**\n",
        "\n",
        "Different because now the prompt is not a parameter, instead an messages structure should be sent. Where the messages will be defined using \"role\" and \"content\"."
      ],
      "metadata": {
        "id": "w4m01ADxl9Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-3.5-turbo\",\n",
        "                                 temperature=0,\n",
        "                                 max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "TJYHMHvpmA2D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who\\\n",
        " responds in a style of Shakespeare\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem\\\n",
        " about a story of success\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzR5nguTmaKx",
        "outputId": "29a46250-2df8-4dd9-bdd0-ec721f6ecdb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O, sweet success, how fair thou art!\n",
            "A tale of toil, with steadfast heart\n",
            "Does lead one to prosperity's shore,\n",
            "And now, the fruits of labor pour.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# length\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':'All your responses must be \\\n",
        "one sentence long.'},\n",
        "{'role':'user',\n",
        " 'content':'write me a story of success'},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature =1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FhT48CenLVS",
        "outputId": "93900ce0-1858-433f-f5f1-697118b50cb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After years of hard work and perseverance, Jane's startup grew into a successful business, allowing her to retire at 40 and travel the world with her family.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combined\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who \\\n",
        "responds in a style of Shakespeare \\\n",
        "All your responses must be one sentence long.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a story of success\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages,\n",
        "                                        temperature =1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AaDpZMDndXY",
        "outputId": "7d530471-3418-4729-a854-d5ee7a7164cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once there was a lad, named John, who faced many difficulties, but he persevered, worked arduously, and with toil and sweat, he eventually triumphed over his obstacles and achieved great success, becoming a renowned lawyer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing the Tokens**\n",
        "\n",
        "Here we have a method which also give us the token dictionary."
      ],
      "metadata": {
        "id": "tLjaV0XwoPhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-3.5-turbo\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message[\"content\"]\n",
        "\n",
        "    token_dict = {\n",
        "'prompt_tokens':response['usage']['prompt_tokens'],\n",
        "'completion_tokens':response['usage']['completion_tokens'],\n",
        "'total_tokens':response['usage']['total_tokens'],\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ],
      "metadata": {
        "id": "7fB9DLieoaux"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who\\\n",
        " responds in a style of Shakespeare\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem\\\n",
        " about a story of success\"\"\"},\n",
        "]\n",
        "response, token_dict = get_completion_and_token_count(messages, temperature=1)\n",
        "print(response)\n",
        "print(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnZ93C-ronlU",
        "outputId": "9a3d075b-8047-40fe-95c4-ee417740c290"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, mark my words and heed my tale,\n",
            "Of one who made their dream prevail.\n",
            "Though tides may turn and winds may blow,\n",
            "Their passion burned with steady glow.\n",
            "\n",
            "With every setback, they did learn,\n",
            "To rise again and brighter burn.\n",
            "Through every hardship, they did strive,\n",
            "Their goal in sight and strength alive.\n",
            "\n",
            "And when at last they reached the peak,\n",
            "Their victory was sweet to speak.\n",
            "For though their journey hard-pressed,\n",
            "They never strayed from their request.\n",
            "\n",
            "A story of success they wrote,\n",
            "Through strength of heart and noble note.\n",
            "And let it be a lesson true,\n",
            "That all our dreams, we can pursue.\n",
            "{'prompt_tokens': 35, 'completion_tokens': 130, 'total_tokens': 165}\n"
          ]
        }
      ]
    }
  ]
}